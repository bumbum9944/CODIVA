{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd07556cdbc131fc7d669016546e55f6d6ec71a0c9e0e1a1df7e6659fff6f988da5",
   "display_name": "Python 3.7.10 64-bit ('venv2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "import os, warnings\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "# import tf\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "\n",
    "# import image utilities\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"deepfashion_train\",)\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 13\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.65 # set threshold for this model\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"/data/deepfashion2/models/second_model/model_final.pth\"\n",
    "cfg.MODEL.DEVICE='cpu'\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image):\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    if w > h:\n",
    "        ratio = 299.0 / w\n",
    "        x = 299\n",
    "        y = int(h * ratio)\n",
    "        dim = (x, y)\n",
    "    else:\n",
    "        ratio = 299.0 / h\n",
    "        y = 299\n",
    "        x = int(w * ratio)\n",
    "        dim = (x, y)\n",
    "\n",
    "    # perform the actual resizing of the image\n",
    "    resized = cv2.resize(image, dim, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    if resized.shape[0] > resized.shape[1]:                            \n",
    "    # numpy로 검은색 배경을 생성                                            \n",
    "        background = np.zeros((resized.shape[0],resized.shape[0], 3),dtype=np.uint8) \n",
    "     # 이미지를 가운데로 겹치도록 설정\n",
    "        x_offset = y_offset = resized.shape[0] // 2 - (resized.shape[1] // 2)   \n",
    "    # 해당 위치에 이미지를 겹침                    \n",
    "        background[:, x_offset:x_offset + resized.shape[1]] = resized                              \n",
    "    else:\n",
    "    # 가로가 더 길 경우\n",
    "        background = np.zeros((resized.shape[1], resized.shape[1], 3),dtype=np.uint8) \n",
    "        x_offset = y_offset = resized.shape[1] // 2 - (resized.shape[0] // 2)\n",
    "        background[y_offset:y_offset + resized.shape[0],:] = resized\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/azure/ai/data/clasification/train_images'\n",
    "dirs = os.listdir(path)\n",
    "output_path = '/home/azure/ai/data/train_crop_images'\n",
    "os.chdir(output_path)\n",
    "for dir in dirs:\n",
    "    if not os.path.exists(dir):\n",
    "            os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-48a99d8a9730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/envs/venv2/lib/python3.7/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;31m# whether the model expects BGR inputs or RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0moriginal_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "path = '/home/azure/ai/data/clasification/train_images'\n",
    "output_path = '/home/azure/ai/data/train_crop_images'\n",
    "dirs = os.listdir(path)\n",
    "for dir in dirs:\n",
    "    img_names = os.listdir(path + '/' + dir)\n",
    "    for name in img_names:\n",
    "        im = cv2.imread(path + '/' + dir + '/' + name)\n",
    "        outputs = predictor(im)\n",
    "    \n",
    "        boxes = {}\n",
    "        for coordinates in outputs[\"instances\"].to(\"cpu\").pred_boxes:\n",
    "            coordinates_array = []\n",
    "            for k in coordinates:\n",
    "                coordinates_array.append(int(k))\n",
    "            boxes[uuid.uuid4().hex[:].upper()] = coordinates_array\n",
    "            \n",
    "        name = name.replace('.jpg', '')\n",
    "        j = 1    \n",
    "        for k,v in boxes.items():\n",
    "            crop_img = im[v[1]:v[3], v[0]:v[2], :]\n",
    "            crop_img = resize_image(crop_img)\n",
    "            cv2.imwrite(os.path.join(output_path + '/' + dir, name + '_' + str(j) + '.jpg'), crop_img)\n",
    "            j += 1"
   ]
  }
 ]
}